{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from scipy.stats import binomtest, mannwhitneyu, wilcoxon, ttest_1samp\n",
    "import statsmodels.stats.weightstats as sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# verizon\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "# verizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ng/f8vsqm9539z8fn3nw5gt9whh0000gn/T/ipykernel_14007/1136338992.py:7: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  clec_df.mean() - ilec_df.mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Time    8.09752\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/verizon.txt', sep='\\t')\n",
    "\n",
    "# task 1: Calculate the difference in mean repair times for ILEC and CLEC customers. Provide the answer rounded with 1 decimal point.\n",
    "clec_df = df[df['Group'] == 'CLEC']\n",
    "ilec_df = df[df['Group'] == 'ILEC']\n",
    "\n",
    "clec_df.mean() - ilec_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.105"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start by testing the hypothesis that average repair time for CLEC customers is 8 hours or less against the alternative that it's greater\n",
    "#  than 8 hours. Use sign test, provide the p-value rounded to 4 decimal points.\n",
    "\n",
    "np.round(binomtest(len(clec_df[clec_df['Time'] > 8]), len(clec_df), 0.5, alternative='greater').pvalue, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0299"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's compare averages in two samples! \n",
    "# Use t-test to test the hypothesis of equal means against the alternative that mean repair time for CLEC customers is higher. \n",
    "# Provide the p-value rounded to 4 decimal points.\n",
    "\n",
    "np.round(sts.ttest_ind(clec_df.Time, ilec_df.Time, alternative='larger', usevar='unequal')[1], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00046"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Moving on to the rank test for equal averages. Calculate the p-value, round the answer to 5 decimal points.\n",
    "\n",
    "mannwhitneyu(clec_df['Time'], ilec_df['Time'], alternative='greater')[1].round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0179"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Great, let's proceed to the permutation test with the difference of sample means as a statistic. What is it's p-value? Round the answer to 4 decimal points. The sample is too big to go through all the permutation â€“ let's use 10000 of them. To get the same result as us, use the functions from the example notebook, and set random seed = 0 before calling permutation_test_2s function.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def permutation_t_stat_2s(sample1, sample2):\n",
    "    return np.mean(sample1) - np.mean(sample2)\n",
    "\n",
    "def get_random_combinations(n1, n2, max_permutations):\n",
    "    index = np.array(range(n1 + n2))\n",
    "    indices = set([tuple(index)])\n",
    "    for i in range(max_permutations - 1):\n",
    "        np.random.shuffle(index)\n",
    "        indices.add(tuple(index))\n",
    "    return [(index[:n1], index[n1:]) for index in indices]\n",
    "\n",
    "def permutation_null_dist_2s(sample1, sample2, max_permutations = None):\n",
    "    pooled_sample = np.hstack((sample1, sample2))\n",
    "    n1 = len(sample1)\n",
    "    n2 = len(sample2)\n",
    "    n = n1 + n2\n",
    "    \n",
    "    if max_permutations:\n",
    "        indices = get_random_combinations(n1, n2, max_permutations)\n",
    "    else:\n",
    "        indices = [(list(index), filter(lambda i: i not in index, range(n))) \\\n",
    "                    for index in itertools.combinations(range(n), n1)]\n",
    "    \n",
    "    distr = [permutation_t_stat_2s(pooled_sample[list(i[0])], pooled_sample[list(i[1])]) \\\n",
    "             for i in indices]\n",
    "    return distr\n",
    "\n",
    "def permutation_test_2s(sample1, sample2, max_permutations = None, alternative = 'two-sided', return_distr = False):\n",
    "    if alternative not in ('two-sided', 'less', 'greater'):\n",
    "        raise ValueError(\"alternative not recognized\\n\"\n",
    "                         \"should be 'two-sided', 'less' or 'greater'\")\n",
    "    \n",
    "    t_stat = permutation_t_stat_2s(sample1, sample2)\n",
    "    \n",
    "    null_distr = permutation_null_dist_2s(sample1, sample2, max_permutations)\n",
    "    \n",
    "    if alternative == 'two-sided':\n",
    "        p = sum([1. if abs(x) >= abs(t_stat) else 0. for x in null_distr]) / len(null_distr)\n",
    "    elif alternative == 'less':\n",
    "        p = sum([1. if x <= t_stat else 0. for x in null_distr]) / len(null_distr)\n",
    "    else: # alternative == 'greater':\n",
    "        p = sum([1. if x >= t_stat else 0. for x in null_distr]) / len(null_distr)\n",
    "    \n",
    "    if return_distr:\n",
    "        return {'t': t_stat, 'p': p, 'null_distr': null_distr}\n",
    "    else:\n",
    "        return {'t': t_stat, 'p': p}\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "np.round(permutation_test_2s(clec_df['Time'], ilec_df['Time'], max_permutations=10000, alternative='greater')['p'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# software reliability\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "# software reliability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Students t-test res: 0.0406\n"
     ]
    }
   ],
   "source": [
    "# Do failures on average happen more often than every 500 CPU seconds? Let's test the following hypothesis:\n",
    "\n",
    "# ð»0: average time between failures is not greater than 500 CPU seconds\n",
    "# ð»1: average time between failures is greater than 500 CPU seconds\n",
    "\n",
    "# # First, let's use Student's t-test. What is its p-value? Round the answer to 4 decimal points.\n",
    "\n",
    "df = pd.read_csv('../datasets/failure_times.txt', sep='\\t', header=None)\n",
    "df.columns = ['time']\n",
    "df['shifted'] = df.time.shift(1)\n",
    "df['dif'] = df.time - df.shifted\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(f\"Students t-test res: {np.round(ttest_1samp(df.dif, 500, alternative='greater').pvalue, 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num obs w dif > 500: 49\n"
     ]
    }
   ],
   "source": [
    "# what number of observations in the sample is above 500?\n",
    "dif = len(df[df.dif > 500])\n",
    "print(f\"Num obs w dif > 500: {dif}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sign test res: 0.9995\n"
     ]
    }
   ],
   "source": [
    "# What is the p-value of the sign test? Round the answer to 4 decimal points.\n",
    "print(f\"Sign test res: {np.round(binomtest(dif, len(df), alternative = 'greater').pvalue, 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signed rank test res: 0.8632\n"
     ]
    }
   ],
   "source": [
    "print(f\"Signed rank test res: {np.round(wilcoxon(df.dif - 500, alternative='greater').pvalue, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation test res: 0.0366\n"
     ]
    }
   ],
   "source": [
    "# Great, let's proceed to the permutation test with sum of the (centered) sample as a statistic. What is it's p-value? Round the answer to 4 decimal points.\n",
    "\n",
    "# The sample is too big to go through all the permutation â€“ let's use 10000 of them. \n",
    "# To get the same result as us, use the functions from the example notebook, and set random seed = 0 before calling permutation_test_1s function.\n",
    "\n",
    "def permutation_t_stat_1s(sample, mean):\n",
    "    t_stat = sum(sample - mean)\n",
    "    return t_stat\n",
    "\n",
    "def permutation_null_distr_1s(sample, mean, max_permutations = None):\n",
    "    centered_sample = sample - mean\n",
    "    if max_permutations:\n",
    "        signs_array = set([tuple(x) for x in 2 * np.random.randint(2, size = (max_permutations, \n",
    "                                                                              len(sample))) - 1 ])\n",
    "    else:\n",
    "        signs_array =  product([-1, 1], repeat = len(sample))\n",
    "    distr = [permutation_t_stat_1s(centered_sample * np.array(signs), 0) for signs in signs_array]\n",
    "    return distr\n",
    "\n",
    "def permutation_test_1s(sample, mean, max_permutations = None, alternative = 'two-sided', return_distr = False):\n",
    "    if alternative not in ('two-sided', 'less', 'greater'):\n",
    "        raise ValueError(\"alternative not recognized\\n\"\n",
    "                         \"should be 'two-sided', 'less' or 'greater'\")\n",
    "    \n",
    "    t_stat = permutation_t_stat_1s(sample, mean)\n",
    "    \n",
    "    null_distr = permutation_null_distr_1s(sample, mean, max_permutations)\n",
    "    \n",
    "    if alternative == 'two-sided':\n",
    "        p = sum([1. if abs(x) >= abs(t_stat) else 0. for x in null_distr]) / len(null_distr)\n",
    "    elif alternative == 'less':\n",
    "        p = sum([1. if x <= t_stat else 0. for x in null_distr]) / len(null_distr)\n",
    "    else: # alternative == 'greater':\n",
    "        p = sum([1. if x >= t_stat else 0. for x in null_distr]) / len(null_distr)\n",
    "        \n",
    "    if return_distr:\n",
    "        return {'t': t_stat, 'p': p, 'null_distr': null_distr}\n",
    "    else:\n",
    "        return {'t': t_stat, 'p': p}\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "print(f\"Permutation test res: {permutation_test_1s(np.array(df.dif), mean = 500, max_permutations=10000, alternative = 'greater')['p']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
